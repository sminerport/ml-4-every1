{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755d1c2e",
   "metadata": {},
   "source": [
    "# 15.5 Neural Networks\n",
    "\n",
    "Neural networks start by making connections betwen inputs, mathematical functions, and outputs, and then implement them in code, which they do in very generic ways.\n",
    "\n",
    "## 15.5.1 A NN View of Linear Regression\n",
    "\n",
    "Linear regression will be our prototype for developing a neural network.To implement a neural network, we will use `keras`, which trickers the lower-level neural network package called TensorFlow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26850330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers as kl\n",
    "import keras.models as km\n",
    "import keras.optimizers as ko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bce29",
   "metadata": {},
   "source": [
    "There are a number of ways to define and build `keras` models. We use a simple one that allows us to `add` layers to a model as we work forward, from left to right or from input to output. The process is sequential and moves strictly forward. \n",
    "\n",
    "It turns out that classic linear regression requires only one *work* layer. The work layer connects the inputs and the outputs, which are layers in their own right, but we are not concerned with them since they are fixed in place. In Keras, we define the work layer as a `Dense` layer, which means we connect *all* of the incoming components together (densely). We create it as a `Dense(1)` layer, meaning that we connect all incoming components together into *one* node. The values funnel through the node and form a dot product, which was specified when we said `activation='linear'`.\n",
    "\n",
    "The last piece is how we're going to optimize our model. We define our cost/loss function and a technique to minimize it. We use a mean-squared-error loss(`loss='mse'`) and a rolling-downhill optimizer called *stochastic gradient descent* (`ko.SGD`). The *gradient descent* is the downhill part. *Stochastic* means that we only use of part of our training data, instead of *all* of it, every time we try to figure out the downhill direction. The technique is useful when we have a mammoth amount of training data. We use `lr` (learning rate) in the same way we used `step_size`. Once we know a direction, we need to know how far to move in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487d8e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Keras_LinearRegression(n_ftrs):\n",
    "    model = km.Sequential()\n",
    "    # Dense layer defaults include a \"bias\" (a +1 trick)\n",
    "    model.add(kl.Dense(1,\n",
    "                       activation='linear',\n",
    "                       input_dim=n_ftrs))\n",
    "    model.compile(optimizer=ko.SGD(lr=0.01), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330deb5a",
   "metadata": {},
   "source": [
    "The `keras` developers gave their models the same basic API as `sklearn`. A quick `fit` and `predict` get us predicitons. One slight difference is that `fit` returns a history of the fitting run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e62e6c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linreg_ftrs_p1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6772/2530475352.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# for various reasons, we are going to let Keras do the +1 trick\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# we will *not* send the `ones` feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlinreg_ftrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinreg_ftrs_p1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlinreg_nn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeras_LinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'linreg_ftrs_p1' is not defined"
     ]
    }
   ],
   "source": [
    "# for various reasons, we are going to let Keras do the +1 trick\n",
    "# we will *not* send the `ones` feature\n",
    "linreg_ftrs = linreg_ftrs_p1[:,0]\n",
    "\n",
    "linreg_nn = Keras_LinearRegression(1)\n",
    "history = linreg_nn.fit(linreg_ftrs, linreg_tgt, epochs=1000, verbose=0)\n",
    "preds = linreg_nn.predict(linreg_ftrs)\n",
    "\n",
    "mse = metrics.mean_squared_error(preds, linreg_tgt)\n",
    "\n",
    "print(\"Training MSE: {:5.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee909e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.optimizers' has no attribute 'SGD'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6772/3766238675.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mlogreg_nn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeras_LogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogreg_ftr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogreg_tgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6772/3766238675.py\u001b[0m in \u001b[0;36mKeras_LogisticRegression\u001b[1;34m(n_ftrs)\u001b[0m\n\u001b[0;32m      4\u001b[0m                        \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                        input_dim=n_ftrs))\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mko\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.optimizers' has no attribute 'SGD'"
     ]
    }
   ],
   "source": [
    "def Keras_LogisticRegression(n_ftrs):\n",
    "    model = km.Sequential()\n",
    "    model.add(kl.Dense(1,\n",
    "                       activation='sigmoid',\n",
    "                       input_dim=n_ftrs))\n",
    "    model.compile(optimizer=ko.SGD(), loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "logreg_nn = Keras_LogisticRegression(1)\n",
    "history = logreg_nn.fit(logreg_ftr, logreg_tgt, epochs=1000, verbose=0)\n",
    "\n",
    "# output is a probability\n",
    "preds = logreg_nn.predict(logreg_ftr) > .5\n",
    "print('accuracy:', metrics.accuracy_score(preds, logreg_tgt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
